{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d4cfeb",
   "metadata": {},
   "source": [
    "# Lu Yin Text Analysis Project\n",
    "Stephanie Shu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f46ae",
   "metadata": {},
   "source": [
    "# Framing the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbeb36",
   "metadata": {},
   "source": [
    "## 1. Research interests\n",
    "\n",
    "My primary research involves early twentieth century Chinese literature, specifically focusing representations of female-female homoeroticism. This project aims to extend a year-long thesis project I worked on in 2022, which used traditional close-reading strategies to analyze a set of five woman-authored short stories that depicted female-female homoerotic relationships. As a new programmer, the goal of this project is to allow me to practice using different basic text analysis strategies and evaluate their application to Chinese texts. I approach this project as more of an exercise in exploration and discovery, rather than something with a set path of methods and goals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d58d5",
   "metadata": {},
   "source": [
    "## 2. Who was Lu Yin 廬隱?\n",
    "\n",
    "Lu Yin 廬隱 (1898-1934) was a Chinese author whose stories focused on issues of women's societal entrapment and frustrations with heteronormative marriage culture. In her personal life, Lu Yin was well-known for her close relationship with another female writer, which was certainly homosocial, if not homoerotic in nature. Thus, her stories also explore the intimate and close relationships formed between young women, and the issues of family and national progress that they feel obligated to prioritize over their own happiness, relationships, and even love. \n",
    "\n",
    "Because of these features, I find Lu Yin to be the most compelling of the authors that I studied in my thesis, and therefore the author that I wanted to explore more through computational analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae8668",
   "metadata": {},
   "source": [
    "## 3. Research question\n",
    "\n",
    "As mentioned about, this project is one of curiosity and exploration. My goal is to familiarize myself with the most basic of Chinese text analysis tools, and see what is possible. Some of the questions that guiding this project included:\n",
    "* What information can be gleaned about Lu Yin's works by applying different forms of text analysis? For example, does word frequency analysis provide a good picture of the texts? \n",
    "* What is the best way to deal with Chinese text, which is a character-based language?\n",
    "* Does Latent Dirichlet Allocation-based topic modeling give us better results than character-based or word-based frequency analyses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f80e7",
   "metadata": {},
   "source": [
    "## 4. The Data\n",
    "\n",
    "I used three of Lu Yin's stories in this project:\n",
    "* \"Lishi's Diary\" 麗石的日記 (short story)\n",
    "* <i> Old Friends by the Seashore</i> 海濱故人 (novella)\n",
    "* \"Drifting Women\" 飄泊的女兒 (short story)\n",
    "\n",
    "The texts are sourced from the final copy of my thesis. These were adapted from online versions, which I manually corrected using published print copies. I made a separate text file for each story and uploaded these to github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f31c4",
   "metadata": {},
   "source": [
    "# Project Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bf44a",
   "metadata": {},
   "source": [
    "## 1. Download files\n",
    "1. Go to https://github.com/sshu99/luyin.git \n",
    "2. Open terminal and type 'git clone https://github.com/sshu99/luyin.git'\n",
    "OR\n",
    "2. Download ZIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46089b37",
   "metadata": {},
   "source": [
    "## 2. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9aafaf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zhon in /opt/anaconda3/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: jieba in /opt/anaconda3/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install zhon\n",
    "!pip install jieba\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e6552",
   "metadata": {},
   "source": [
    "## 3. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f26a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stephanie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/stephanie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform \n",
    "import pandas as pd\n",
    "\n",
    "# Python library 'zhon' with constants for Chinese text processing.\n",
    "# https://pypi.org/project/zhon/ \n",
    "import zhon.hanzi\n",
    "\n",
    "# Python module 'string' for removing English punctuation.\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "# Python module 'Counter' for counting most frequent words. \n",
    "from collections import Counter \n",
    "\n",
    "#Python module 'jieba' for segmenting Chinese texts\n",
    "import jieba\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf442b5b",
   "metadata": {},
   "source": [
    "## 4. Open the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a25e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assigning variables to read each file\n",
    "lishi = open('lishi.txt', encoding = 'utf-8').read()\n",
    "haibin = open('haibin.txt', encoding = 'utf-8').read()\n",
    "piaobo = open('piaobo.txt', encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a1bce",
   "metadata": {},
   "source": [
    "## 5. Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9042802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日春雨不住響地滴著，窗外天容愔淡，耳邊風聲淒厲，我靜坐幽齋，思潮起伏，只覺悵然惘然！\n",
      "　　去年的今天，正是我的朋友麗石超脫的日子，現在春天已經回來了，並且一樣的風淒雨冷，但麗石那慘白梨花般的兩靨，誰知變成什麼樣了！\n",
      "　　麗石的死，醫生說是心臟病，但我相信麗石確是死於心病，不是死於身病，她留下的日記，可以證實，現在我將她的日記發表了吧！\n",
      "\n",
      "十二月二十一日\n",
      "　　不記日記已經半年了。只感覺著學校的生活單調，吃飯，睡覺，板滯的上課，教員戴上道德的假面具，像俳優般舞著唱著，我們便像傻子般看著聽著，真是無聊極了。\n",
      "　　圖書館裡，擺滿了古人的陳跡，我掀開了屈原的《離騷》念了幾頁，心竊怪其愚——懷王也值得深\n"
     ]
    }
   ],
   "source": [
    "# Checking that we can read one of the files\n",
    "print(lishi[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432d7b5",
   "metadata": {},
   "source": [
    "# Processing texts for word frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9212de",
   "metadata": {},
   "source": [
    "## 1. Removing punctuation\n",
    "\n",
    "Chinese text uses different punctuation from English, which is why I needed to import a special library with Chinese punctuation earlier. Since people now use English input alongside Chinese input, the text may also contain English punctuation, so I have to remove both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe81d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､　、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·．！？｡。\n"
     ]
    }
   ],
   "source": [
    "# Printing Chinese punctuation\n",
    "print(zhon.hanzi.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec8083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Printing English punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f47865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a For loop to remove Chinese punctuation\n",
    "for char in zhon.hanzi.punctuation:\n",
    "    lishi = lishi.replace(char, '')\n",
    "    haibin = haibin.replace(char, '')\n",
    "    piaobo = piaobo.replace(char, '')\n",
    "\n",
    "# Using a For loop to remove English punctuation\n",
    "for char in string.punctuation:\n",
    "    lishi = lishi.replace(char, '')\n",
    "    haibin = haibin.replace(char, '')\n",
    "    piaobo = piaobo.replace(char, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d475ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日春雨不住響地滴著窗外天容愔淡耳邊風聲淒厲我靜坐幽齋思潮起伏只覺悵然惘然\n",
      "去年的今天正是我的朋友麗石超脫的日子現在春天已經回來了並且一樣的風淒雨冷但麗石那慘白梨花般的兩靨誰知變成什麼樣了\n",
      "麗石的死醫生說是心臟病但我相信麗石確是死於心病不是死於身病她留下的日記可以證實現在我將她的日記發表了吧\n",
      "\n",
      "十二月二十一日\n",
      "不記日記已經半年了只感覺著學校的生活單調吃飯睡覺板滯的上課教員戴上道德的假面具像俳優般舞著唱著我們便像傻子般看著聽著真是無聊極了\n",
      "圖書館裡擺滿了古人的陳跡我掀開了屈原的離騷念了幾頁心竊怪其愚懷王也值得深戀嗎\n",
      "下午回家寂悶更甚這時的心緒真微玄至不可捉摸日來絕要自制不讓消極的思想入據靈台所以\n"
     ]
    }
   ],
   "source": [
    "# Testing to make sure the punctuation has been removed.\n",
    "print(lishi[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abeb859",
   "metadata": {},
   "source": [
    "## 2. Removing spaces\n",
    "Because I want to count word frequency, I also need to remove any spaces between characters so that these aren't in the way of the text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8aeb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing '\\n' and '\\r' to get rid of line breaks.\n",
    "# Replacing ' ' spaces to get rid of any single spaces.\n",
    "lishi = lishi.replace('\\n', '').replace('\\r', '').replace(' ', '')\n",
    "haibin = haibin.replace('\\n', '').replace('\\r', '').replace(' ', '')\n",
    "piaobo = piaobo.replace('\\n', '').replace('\\r', '').replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c083809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日春雨不住響地滴著窗外天容愔淡耳邊風聲淒厲我靜坐幽齋思潮起伏只覺悵然惘然去年的今天正是我的朋友麗石超脫的日子現在春天已經回來了並且一樣的風淒雨冷但麗石那慘白梨花般的兩靨誰知變成什麼樣了麗石的死醫生說是心臟病但我相信麗石確是死於心病不是死於身病她留下的日記可以證實現在我將她的日記發表了吧十二月二十一日不記日記已經半年了只感覺著學校的生活單調吃飯睡覺板滯的上課教員戴上道德的假面具像俳優般舞著唱著我們便像傻子般看著聽著真是無聊極了圖書館裡擺滿了古人的陳跡我掀開了屈原的離騷念了幾頁心竊怪其愚懷王也值得深戀嗎下午回家寂悶更甚這時的心緒真微玄至不可捉摸日來絕要自制不讓消極的思想入據靈台所以又忙把案頭的奮\n"
     ]
    }
   ],
   "source": [
    "# Testing to make sure all spaces have been removed.\n",
    "print(lishi[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399de0a4",
   "metadata": {},
   "source": [
    "## 3. Create a stopword list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218bb6a",
   "metadata": {},
   "source": [
    "To do word frequency analysis, I need to remove common, but relatively irrelevant words and phrases from the text. I searched for a suitable list of stopwords online and came upon this library: https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library. However, the contents were not exactly what I wanted, so I copied it into a new text file and made some edits to it. Now I will open that file in this notebook so I can use this list to remove the stopwords from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eaa72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = open('stopwords.txt', encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80ed809",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '得', '不', '沒', '了', '是', '到', '至', '有', '此', '些', '這', '那', '在', '裡', '外', '上', '下', '著', '者', '個', '也', '來', '去', '和', '只', '都', '就', '之', '過', '地', '無', '對', '又', '起', '前', '能', '以', '但', '而', '並', '可', '麼', '什麼', '因', '因為', '多', '少', '出', '很', '太', '走', '從', '如', '才', '呢', '吧', '啊', '呵', '嗎', '還', '把', '欸', '所以', '何', '結果', '如果', '已經', '已', '向', '最', '作', '做', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '每', '便一個', '一些', '一何', '一切', '一則', '一方面', '一旦', '一來', '一樣', '一種', '一般', '一轉眼', '萬一', '上', '上下', '下']\n"
     ]
    }
   ],
   "source": [
    "# Formatting the text into a list\n",
    "stoplist = stops.replace('\\n','').replace(\"'\", '').replace(\" \", '')\n",
    "stps = stoplist.split(',')\n",
    "\n",
    "# Adding more words to the stopword list as needed\n",
    "stps.append('麼')\n",
    "stps.append('麽') # for some reason there are two unicodes for this character, so we need to add both\n",
    "stps.append('事')\n",
    "stps.append('便')\n",
    "stps.append('見')\n",
    "stps.append('頭')\n",
    "stps.append('子')\n",
    "stps.append('然')\n",
    "stps.append('樣')\n",
    "stps.append('裏')\n",
    "print(stps[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac57703",
   "metadata": {},
   "source": [
    "# >>> Side quest: character name frequencies\n",
    "## 1. Make character lists\n",
    "While creating my stopwords list, I also realized there were character names that I wanted to remove, but I want to remove them separately so I can also count their frequencies. I created lists of characters for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e45fb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "lishi_names = ['麗石', '沅青', '雯薇', '酈文']\n",
    "haibin_names = ['露沙', '雲青', '宗瑩', '玲玉', '蓮裳', '梓青', '劍卿', '師旭']\n",
    "piaobo_names = ['畏如', '星若', '星呵']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cff07f",
   "metadata": {},
   "source": [
    "## 2. Add character lists to jieba dictionary\n",
    "I will be using the jieba Python module to split my texts and search for character names. These character names aren't in the default jieba dictionary, however, so I need to modify the dictionary to recognize the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3d99018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/jt/fyzwsnw901g8rw4fc1nwbh_c0000gn/T/jieba.cache\n",
      "Loading model cost 0.564 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Writing a function to add the names from the list to jieba\n",
    "def add_words_from(namelist):\n",
    "    for word in namelist:\n",
    "        jieba.add_word(word)\n",
    "\n",
    "# Using the function to add each list of names to jieba for all three texts\n",
    "add_words_from(lishi_names)\n",
    "add_words_from(haibin_names)\n",
    "add_words_from(piaobo_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d519f",
   "metadata": {},
   "source": [
    "## 3. Write a function to return character counts\n",
    "\n",
    "Now the set-up is ready, and I can write a function to process the text and output a dataframe with the frequencies of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbb1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to produce the correct names list \n",
    "# For use in the following function\n",
    "def producelist(text):\n",
    "    if text == lishi:\n",
    "        return lishi_names\n",
    "    if text == haibin:\n",
    "        return haibin_names\n",
    "    if text == piaobo:\n",
    "        return piaobo_names\n",
    "\n",
    "# Writing a function to list the frequency of each character's name\n",
    "def name_counter(text):\n",
    "    seg_text = jieba.cut(text, cut_all=False) # Using jieba to segment the text\n",
    "    c = Counter(seg_text)\n",
    "    new_list = {}\n",
    "    names_list = producelist(text)\n",
    "    for name in names_list: # Using a For loop to check each of the names in the list\n",
    "        new_list[name] = c[name]\n",
    "    name_freq = sorted(new_list.items(), key=lambda x:x[1], reverse = True) \n",
    "        # Sorting the names by frequency\n",
    "    return pd.DataFrame(name_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dccef5f",
   "metadata": {},
   "source": [
    "## 4. Counting name frequencies in each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8911a27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>沅青</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>麗石</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>雯薇</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>酈文</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  沅青  38\n",
       "1  麗石  14\n",
       "2  雯薇   9\n",
       "3  酈文   3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_counter(lishi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c51b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>露沙</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>雲青</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>宗瑩</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>玲玉</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>梓青</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>蓮裳</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>劍卿</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>師旭</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1\n",
       "0  露沙  285\n",
       "1  雲青  117\n",
       "2  宗瑩  109\n",
       "3  玲玉   84\n",
       "4  梓青   69\n",
       "5  蓮裳   25\n",
       "6  劍卿   17\n",
       "7  師旭    5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_counter(haibin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a7b185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>畏如</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>星若</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>星呵</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  畏如  22\n",
       "1  星若  19\n",
       "2  星呵   4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_counter(piaobo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda6e87",
   "metadata": {},
   "source": [
    "This side quest is not particularly relevant to my research question, but it is interesting to see which characters' names are mentioned the most! A different research question that asks about characters and their relationships might benefit from something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62a8af",
   "metadata": {},
   "source": [
    "## 5. Removing names\n",
    "\n",
    "And now that we have the name frequencies, we can take the names out of the text so they won't be counted in the word frequencies later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f095dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to remove the characters' names from each text\n",
    "# nc stands for 'no characters'\n",
    "\n",
    "def nc(text):\n",
    "    namelist = producelist(text)\n",
    "    for name in namelist:\n",
    "        if name in text:\n",
    "            text = text.replace(name,'')\n",
    "    return text\n",
    "\n",
    "nc_lishi = nc(lishi)\n",
    "nc_haibin = nc(haibin)\n",
    "nc_piaobo = nc(piaobo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015670c6",
   "metadata": {},
   "source": [
    "### Now back to the main quest...\n",
    "# Exploring basic word frequencies\n",
    "Now that I have finished removing punctuation, space, and character names, I will get back to the main goal of exploring word frequencies. First, I need to remove stopwords so I can look at the word frequencies for just the important words in each text. Then, I can write a function to calculate those frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782081bc",
   "metadata": {},
   "source": [
    "## 1. Removing stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cbdaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nc_lishi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(stop,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 10\u001b[0m ns_lishi \u001b[38;5;241m=\u001b[39m ns(nc_lishi)\n\u001b[1;32m     11\u001b[0m ns_haibin \u001b[38;5;241m=\u001b[39m ns(nc_haibin)\n\u001b[1;32m     12\u001b[0m ns_piaobo \u001b[38;5;241m=\u001b[39m ns(nc_piaobo)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nc_lishi' is not defined"
     ]
    }
   ],
   "source": [
    "# Writing a function to remove stopwords from each text\n",
    "# ns stands for 'no stops'\n",
    "\n",
    "def ns(text):\n",
    "    for stop in stps:\n",
    "        if stop in text:\n",
    "            text = text.replace(stop,'')\n",
    "    return text\n",
    "\n",
    "ns_lishi = ns(nc_lishi)\n",
    "ns_haibin = ns(nc_haibin)\n",
    "ns_piaobo = ns(nc_piaobo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc3f10",
   "metadata": {},
   "source": [
    "## 2. Calculating word frequencies \n",
    "\n",
    "Now we can finally look at word frequencies and see which words are the most frequent in each text. We can look at these words and see if they are helpful in either reflecting or evaluating the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68d660e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to produce the correct text with no characters and no stops\n",
    "# For use in the following function\n",
    "def produce_ns(text):\n",
    "    if text == lishi:\n",
    "        return ns_lishi\n",
    "    if text == haibin:\n",
    "        return ns_haibin\n",
    "    if text == piaobo:\n",
    "        return ns_piaobo\n",
    "\n",
    "# Writing a function to return the 10 most common words in a text\n",
    "def word_frequencies(text):\n",
    "    freq = Counter(produce_ns(text)).most_common(10) # Using the Counter module to get the most frequently used words\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c9e09",
   "metadata": {},
   "source": [
    "## 3. Reviewing the common words* in these three texts:\n",
    "*Technically, the function I wrote above calculates character frequencies, not word frequencies. In Chinese, characters have meanings of their own, but they may also appear in other words that are comprised of multiple characters.\n",
    "\n",
    "I've added very rough translations to the dataframe to give non-Chinese speakers a sense of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b309f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('天', 45), ('心', 34), ('生', 33), ('覺', 27), ('真', 22), ('情', 20), ('信', 19), ('病', 18), ('想', 18), ('回', 17)]\n"
     ]
    }
   ],
   "source": [
    "print (word_frequencies(lishi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a8bd4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天</td>\n",
       "      <td>day, heaven, sky</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>心</td>\n",
       "      <td>heart, mind</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>生</td>\n",
       "      <td>life, birth</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>覺</td>\n",
       "      <td>feel</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>真</td>\n",
       "      <td>real, true</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>情</td>\n",
       "      <td>emotion</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>信</td>\n",
       "      <td>believe, letter</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>病</td>\n",
       "      <td>illness, sick</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>想</td>\n",
       "      <td>think, feel, want</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>回</td>\n",
       "      <td>return</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character         definition  frequency\n",
       "0         天   day, heaven, sky         45\n",
       "1         心        heart, mind         34\n",
       "2         生        life, birth         33\n",
       "3         覺               feel         27\n",
       "4         真         real, true         22\n",
       "5         情            emotion         20\n",
       "6         信    believe, letter         19\n",
       "7         病      illness, sick         18\n",
       "8         想  think, feel, want         18\n",
       "9         回             return         17"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "lishi_df = pd.DataFrame(word_frequencies(lishi))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "lishi_df.columns=['character', 'frequency']\n",
    "lishi_df.insert(1, 'definition',['day, heaven, sky', 'heart, mind', 'life, birth', 'feel', 'real, true', 'emotion', 'believe, letter','illness, sick','think, feel, want', 'return'], True) \n",
    "\n",
    "# Printing the dataframe\n",
    "lishi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4307ab",
   "metadata": {},
   "source": [
    "### 3a. Understanding \"Lishi's Diary\" character frequencies\n",
    "\n",
    "\"Lishi's diary\" is a fictional diary that is very emotional. The main character and fictional author, Lishi, writes about her feelings in a lot of depth. She also commonly associates sadness and discontentment with illness, which we can also see reflected in this character count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e47446a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('天', 178), ('道', 160), ('生', 135), ('心', 131), ('情', 112), ('信', 107), ('想', 105), ('家', 99), ('覺', 98), ('回', 90)]\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies(haibin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d924cde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天</td>\n",
       "      <td>day, heaven, sky</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>道</td>\n",
       "      <td>way, channel</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>生</td>\n",
       "      <td>life, birth</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>心</td>\n",
       "      <td>heart, mind</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>情</td>\n",
       "      <td>emotion</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>信</td>\n",
       "      <td>believe, letter</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>想</td>\n",
       "      <td>think, feel, want</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>家</td>\n",
       "      <td>family, home</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>覺</td>\n",
       "      <td>feel</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>回</td>\n",
       "      <td>return</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character         definition  frequency\n",
       "0         天   day, heaven, sky        178\n",
       "1         道       way, channel        160\n",
       "2         生        life, birth        135\n",
       "3         心        heart, mind        131\n",
       "4         情            emotion        112\n",
       "5         信    believe, letter        107\n",
       "6         想  think, feel, want        105\n",
       "7         家       family, home         99\n",
       "8         覺               feel         98\n",
       "9         回             return         90"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "haibin_df = pd.DataFrame(word_frequencies(haibin))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "haibin_df.columns=['character', 'frequency']\n",
    "haibin_df.insert(1, 'definition',['day, heaven, sky', 'way, channel', 'life, birth', 'heart, mind', 'emotion', 'believe, letter', 'think, feel, want', 'family, home', 'feel', 'return'],True)\n",
    "\n",
    "# Printing the dataframe\n",
    "haibin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc11e5d",
   "metadata": {},
   "source": [
    "### 3b. Understanding \"Friends by the Seashore\" character frequencies\n",
    "\n",
    "\"Friends by the Seashore\" (Habin guren) is a novella, so we can see that the character counts are very high. The novella is filled with emotional letters and poems written between characters. The characters also often discuss family obligations and whether or not they will return home. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac1a86bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('心', 18), ('家', 18), ('女', 16), ('男', 16), ('想', 15), ('愛', 12), ('住', 11), ('回', 11), ('海', 9), ('生', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies(piaobo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a3c873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>心</td>\n",
       "      <td>heart, mind</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>家</td>\n",
       "      <td>family, home</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女</td>\n",
       "      <td>woman, female</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>男</td>\n",
       "      <td>man, male</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>想</td>\n",
       "      <td>think, feel, want</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>愛</td>\n",
       "      <td>love</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>住</td>\n",
       "      <td>live, reside</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>回</td>\n",
       "      <td>return</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>海</td>\n",
       "      <td>sea, ocean</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>生</td>\n",
       "      <td>life, birth</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character         definition  frequency\n",
       "0         心        heart, mind         18\n",
       "1         家       family, home         18\n",
       "2         女      woman, female         16\n",
       "3         男          man, male         16\n",
       "4         想  think, feel, want         15\n",
       "5         愛               love         12\n",
       "6         住       live, reside         11\n",
       "7         回             return         11\n",
       "8         海         sea, ocean          9\n",
       "9         生        life, birth          9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "piaobo_df = pd.DataFrame(word_frequencies(piaobo))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "piaobo_df.columns=['character', 'frequency']\n",
    "piaobo_df.insert(1, 'definition',['heart, mind', 'family, home', 'woman, female', 'man, male', 'think, feel, want', 'love', 'live, reside', 'return', 'sea, ocean', 'life, birth'],True)\n",
    "\n",
    "# Printing the dataframe\n",
    "piaobo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0f30d",
   "metadata": {},
   "source": [
    "###  3c. Understanding \"Drifting Women\" character frequencies\n",
    "\n",
    "\"Drifting Women\" (Piaobo de nü'er) is a short story about two women who are not sure if they should follow their familial duty and get married to men, or if they should follow their own desires and love and stay together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea98f4",
   "metadata": {},
   "source": [
    "## 4. Evaluating character frequencies\n",
    "\n",
    "Now that I have found the 10 most common characters across the three texts, I am able to see that the most common words/characters do seem to reflect central concerns from the texts. However, this is more circumstantial evidence, because we already know what the texts are about. Knowing the content of the text helped me interpret why certain characters showed up the most often. \n",
    "\n",
    "But, if I wanted to process an unknown text the same way, these character frequencies probably wouldn't be enough to predict the content of a text. For predictive power, I'm going to need something more advanced than simple counting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf1bf2",
   "metadata": {},
   "source": [
    "# Exploring word frequencies through text segmentation\n",
    "Now, I will try looking at word frequencies through a different lens: text segmentation. Since words can be multiple characters long, I am hoping that using text segmentation will produce informative results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b6e5c",
   "metadata": {},
   "source": [
    "## 1. Writing a function to segment the text and count frequencies\n",
    "\n",
    "I am using the Python module jieba, which is built for Chinese text segmentation. Given the nature of the Chinese language, which does not have prefizes, suffixes, and conjugations, words already hold their original semantic meaning more or less in their form. Thus, we can think of text segmentation as performing a very similar function to that of lemmatization for English texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "860195a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function that will simultaneously segment the text and count frequencies\n",
    "def seg_frequency(text):\n",
    "    ns_text = (''.join(produce_ns(text)))\n",
    "    seg_text = jieba.cut(ns_text, cut_all=False) # Using jieba to segment the text\n",
    "    freq = Counter(seg_text).most_common(10) # Using the Counter module to get the most frequently used words\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3d96f",
   "metadata": {},
   "source": [
    "## 2. Reviewing the common words in these three texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e819cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('安慰', 9), ('生活', 8), ('聽', 8), ('結婚', 7), ('睡', 6), ('寂寞', 6), ('昨天', 5), ('昨夜', 5), ('勉強', 5), ('天津', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(seg_frequency(lishi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96231d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>安慰</td>\n",
       "      <td>comfort, soothe</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>生活</td>\n",
       "      <td>life</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>聽</td>\n",
       "      <td>listen, hear</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>結婚</td>\n",
       "      <td>marry</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>睡</td>\n",
       "      <td>sleep</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>寂寞</td>\n",
       "      <td>lonely, loneliness</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>昨天</td>\n",
       "      <td>yesterday</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>昨夜</td>\n",
       "      <td>last night</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>勉強</td>\n",
       "      <td>do reluctantly, force</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>天津</td>\n",
       "      <td>Tianjin (city name)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character             definition  frequency\n",
       "0        安慰        comfort, soothe          9\n",
       "1        生活                   life          8\n",
       "2         聽           listen, hear          8\n",
       "3        結婚                  marry          7\n",
       "4         睡                  sleep          6\n",
       "5        寂寞     lonely, loneliness          6\n",
       "6        昨天              yesterday          5\n",
       "7        昨夜             last night          5\n",
       "8        勉強  do reluctantly, force          5\n",
       "9        天津    Tianjin (city name)          5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "lishi_seg = pd.DataFrame(seg_frequency(lishi))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "lishi_seg.columns=['character', 'frequency']\n",
    "lishi_seg.insert(1, 'definition',['comfort, soothe', 'life', 'listen, hear', 'marry', 'sleep', 'lonely, loneliness', 'yesterday', 'last night', 'do reluctantly, force', 'Tianjin (city name)'],True)\n",
    "\n",
    "# Printing the dataframe\n",
    "lishi_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e599d34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('道', 60), ('聽', 36), ('話', 35), ('知道', 34), ('吃', 25), ('想', 25), ('生活', 25), ('寫', 24), ('封信', 22), ('朋友', 21)]\n"
     ]
    }
   ],
   "source": [
    "print(seg_frequency(haibin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d23988bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>道</td>\n",
       "      <td>way, path</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>聽</td>\n",
       "      <td>listen, hear</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>話</td>\n",
       "      <td>speech, language</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>知道</td>\n",
       "      <td>know, understand</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>吃</td>\n",
       "      <td>eat</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>想</td>\n",
       "      <td>think, feel, want</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>生活</td>\n",
       "      <td>life</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>寫</td>\n",
       "      <td>write</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>封信</td>\n",
       "      <td>letter</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>朋友</td>\n",
       "      <td>friend</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character         definition  frequency\n",
       "0         道          way, path         60\n",
       "1         聽       listen, hear         36\n",
       "2         話   speech, language         35\n",
       "3        知道   know, understand         34\n",
       "4         吃                eat         25\n",
       "5         想  think, feel, want         25\n",
       "6        生活               life         25\n",
       "7         寫              write         24\n",
       "8        封信             letter         22\n",
       "9        朋友             friend         21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "haibin_seg = pd.DataFrame(seg_frequency(haibin))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "haibin_seg.columns=['character', 'frequency']\n",
    "haibin_seg.insert(1, 'definition',['way, path', 'listen, hear', 'speech, language', 'know, understand', 'eat', 'think, feel, want', 'life', 'write', 'letter', 'friend'],True)\n",
    "\n",
    "# Printing the dataframe\n",
    "haibin_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d57bd222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('想', 8), ('男', 6), ('青春', 5), ('找', 5), ('朋友', 4), ('兩', 4), ('道', 4), ('回家', 4), ('嫁', 4), ('住', 3)]\n"
     ]
    }
   ],
   "source": [
    "print(seg_frequency(piaobo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c49252dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>definition</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>想</td>\n",
       "      <td>think, feel, want</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>男</td>\n",
       "      <td>man, male</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>青春</td>\n",
       "      <td>youth</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>找</td>\n",
       "      <td>find</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>朋友</td>\n",
       "      <td>friend</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>兩</td>\n",
       "      <td>two, pair</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>道</td>\n",
       "      <td>way, path,</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>回家</td>\n",
       "      <td>return home</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>嫁</td>\n",
       "      <td>marry</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>住</td>\n",
       "      <td>live, reside</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  character         definition  frequency\n",
       "0         想  think, feel, want          8\n",
       "1         男          man, male          6\n",
       "2        青春              youth          5\n",
       "3         找               find          5\n",
       "4        朋友             friend          4\n",
       "5         兩          two, pair          4\n",
       "6         道         way, path,          4\n",
       "7        回家        return home          4\n",
       "8         嫁              marry          4\n",
       "9         住       live, reside          3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a variable to the dataframe\n",
    "piaobo_seg = pd.DataFrame(seg_frequency(piaobo))\n",
    "\n",
    "# Naming columns and adding a column with definitions\n",
    "piaobo_seg.columns=['character', 'frequency']\n",
    "piaobo_seg.insert(1, 'definition',['think, feel, want', 'man, male', 'youth', 'find', 'friend', 'two, pair', 'way, path,', 'return home', 'marry', 'live, reside'],True)\n",
    "\n",
    "# Printing the dataframe\n",
    "piaobo_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb0ab",
   "metadata": {},
   "source": [
    "## 3. Evaluating character frequencies\n",
    "\n",
    "The results of the word frequency analysis on the segmented texts produces quite different results from the word frequency analysis that only counted character frequencies. As this experiment shows, there are quite a few seemingly unhelpful words that end up at the top of the lists, such as 吃 \"eat\" and 天津 \"Tianjin\" (city name). Aside from this, however, there are also helpful words that didn't show up in the other character-based frequency analysis. For example, 嫁 \"marry\", 青春 \"youth\", 朋友 \"friend\", 安慰 \"comfort\" are all important aspects to these stories. \n",
    "\n",
    "This does show that the frequencies shift dramatically between the two methods. If you look at how jieba segments a text, it becomes clear why. As seen below, the text is chopped into chunks, with some chunks being two or three characters long. This means that frequent characters aren't considered alone, but rather as part of the word they are found in. Thus, the frequencies will shift when compared to character-based frequency analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e186e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'春雨/住/響/滴/窗/天容/愔/淡/耳/風/聲/淒/厲/靜/坐/幽/齋/思潮/伏/覺/悵/惘/天正/朋友/超/脫/現/春天/回/風/淒/雨/冷/慘/白梨/梨花/般/兩/靨/知/變/成/死/醫/生/心/臟/病/相信/確/死心/心病/病死/身/病/留/記/證/實/現/記/發/表/記/記/半/感/覺/學/校/生活/單/調/吃/飯/睡/覺/板/滯/課/教/員/戴/道德/假面/假面具/面具/俳/優/般/舞/唱/傻/般/聽/真/聊/極/圖/書/館/擺/滿/古/陳/跡/掀/開/屈原/騷/念/頁/心/竊/怪/愚/懷/王/值/深/戀/午/回家/寂/悶/甚/心/緒/真/微/玄/捉摸/絕/制/消/極/思想/想入/靈/台/忙/案/奮/鬥/雜/誌/讀/晚/飯/生/海信/寥寥/行/系心/心坎/流/近/宿/常常/戕/身/白/蘭/酒/兩/天/喝完/瓶/沉醉/忘/憂/候/憐/感情/情海/豈/容/輕/陷/指路/紅/燈/盞/萬/矢/紅/燈/料定/勝/實/海/蘭/女/世界/絕/僅/生/永/遠/瞭/解/層/罷/夜/復/生/信/竟/受困/確/搜/枯/腸/找/句/恰/話/足/安慰/實/真正/苦/悶/候/絕/話/安慰/天/俗例/冬/節/學/堂/放/天/假/早晨/姑母/忙/預/備/祭祖/免/想/家情/緒/憶/獨/異/鄉/異/客/逢/佳/節/倍/思/親/愴/淚/姑丈/老病/兩/天'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = jieba.cut(ns_lishi[:300], cut_all=True)\n",
    "'/'.join(list(TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa7a86",
   "metadata": {},
   "source": [
    "## 4. Comparing the two frequency analysis methods\n",
    "When comparing these two ways of doing frequency analysis, it's hard to say which method does a better job at showing trends in the text. We can try to see if we can compare them by writing a function that matches the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242996f5",
   "metadata": {},
   "source": [
    "### 4a. Get summaries of each text\n",
    "I'll be using the summaries I wrote for each of these texts as the metric for comparison. Obviously, this is a very subjective measure, since I was the one who wrote these summary texts. But hopefully it can give us an idea of how well these frequency counts match the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "336e2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lishi_summary = 'This lesser-known work by Lu Yin is formatted and presented as the diary of a deceased young woman named Lishi 麗石, ostensibly published by a friend who believes she died not of heart disease, but rather of heartsickness. Through the many entries, Lishi’s story of love and loss unfolds. After witnessing the toll marriage has taken on her childhood friend Wenwei 雯薇, Lishi confesses her feelings of disillusionment with marriage to her close friend Yuanqing 沅青. Finding sympathy in one another, the two grow closer, and their relationship evolves into one that Lishi explicitly defines as “same- gender love” 同性的愛戀. The two happily imagine a future together, and that evening, Lishi dreams of traveling down a stream with Yuanqing on a moonlit night. But this happiness does not last long; Yuanqing is called home by her mother, and upon her return, seems more sullen, though she refuses to explain why. The following day, Lishi receives a letter from Yuanqing, saying that her mother has arranged for her to move to Tianjin to spend time with her cousin, who she is expected to marry. Though the initial letter is filled with resentment towards a world that will not allow their love, within a few weeks she sends another letter to Lishi, encouraging her to awaken to reality and the “childishness” of their previous dreams. To add insult to injury, Yuanqing sends a young man to court Lishi, which causes her to sink deeper into melancholia. In the final entry, Lishi wishes that death will come quickly, and with this, her diary ends abruptly.'\n",
    "haibin_summary = 'The opening of Lu Yin’s Old Friends by the Seashore starts with the picturesque scene of five young female friends, Lusha 露莎, Lingyu 玲玉, Yunqing 雲青, Zongying 宗瑩, and Lianshang 蓮裳, spending a relaxing summer along the seashore during their break between academic terms. Having isolated themselves from their peers, their relationship is very intimate and affectionate, and they spend their free time debating various intellectual inquiries, such as boundaries of emotion and logic or the meaning of “love.” Soon after, the girls return to school, and as they approach their graduation, they grow increasingly apprehensive of the oppressive life that awaits them once they enter society and are expected to marry, bear children, and ultimately, sacrifice their individual ambitions for their families. They worry, too, that their education, intellectual enlightenment, and modern values may only worsen their feelings of entrapment once they become captives to society, and Lusha in particular has a mounting fear that life and love may be fundamentally meaningless. Indeed, as time wears on, each woman experiences her own personal tragedies. At Lianshang’s wedding, the other four girls stand miserably to the side as they helplessly witness the festivities which are the first indication of their diverging paths. Lusha’s tragedy is two-fold; the first is her mother’s death, and the second, her continued agony over the impossibility of a relationship with the man she loves, Ziqing, who is still married to a woman his parents chose for him. Yunqing, too, struggles because herfamily disapproves of her suitor. Meanwhile, Zongying and Lingyu each find potential marriage matches, gradually spending less time with the group to accompany their boyfriends. Seeing their increasingly apathetic attitude towards the friendship, Lusha and Yunqing bemoan the impossibility of a dream all four once shared—a home by the sea where they could teach or write and leisurely spend time together, just as they did in days gone by.Their misfortune continues as Zongying, who despite overcoming her family’s disapproval and marrying the man of her choice, falls severely ill less than a month after her wedding. Lingyu has plans to marry, while Yunqing, unwilling to rebel against her family’s wishes, gives up on her love to return home and care for her mother and siblings. With all her friends gone in scattered in directions, Lusha writes to Yunqing, explaining that she and Ziqing, though unable to concretize their love through any legal or socially accepted channels, have decided to build a life together. She writes that they have purchased land to build a small hut along the same seashore where the four women vacationed in their youth, and plan to settle there together—if they can succeed in accomplishing their shared revolutionary goals. If they fail, she writes, they will commit double suicide by treading into the waves. After receiving this letter, Yunqing does not hear from Lusha again.One year later, Lingyu and Yunqing make their way back to the seaside. There they find an empty house, just as Lusha described, with the words “Old Friends by the Seashore” inscribed above the door frame.'\n",
    "piaobo_summary = 'Set in 1932, the story unfolds against the backdrop of Japanese military aggression and general political chaos in Shanghai. The first scene of the story opens with the sound of explosions waking the two protagonists, lovers Weiru 畏如 and Xingruo 星若. The two women have been romantically involved for several years and are currently staying in a friend’s home in Shanghai, searching for jobs. The state of the outside world is reflected in the two protagonists’ individual states. Both are unemployed and struggling to support themselves financially. Moreover, they find themselves having to reckon with the reality of independent life. Weiru, the older of the two, bemoans men’s unwillingness to come to her aid as they had in the past. It is only through Xingruo’s explanation that she realizes their previous attentiveness towards her stemmed not from interest in her as a person, but rather as a sexual object. Now that she is perceived to have passed her so-called prime, men no longer pay her any heed. Comforting Weiru, Xingruo reassures her that their love is more than sufficient. The two decide to make a pact to stay unmarried, and if this proves impossible, they promise to get married at the same time, inthe same place. When the two women find themselves still unemployed, despite their continuous search, they decide to return home to their families. First, they visit Weiru’s elderly parents together, then Xingruo returns to her own hometown by herself. At home, amidst pressure from her family and friends, Xingruo begins to consider marriage and returns to Shanghai to look for potential marriage prospects. She receives a letter from Weiru, in which Weiru swears off love, saying that the age of love has passed with her youth. She writes saying that she must be practical now and put her family’s survival before her own ambitions and dreams. Aimlessly wandering in Shanghai alone, Xingruo thinks of Weiru, drifting alone somewhere far away.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24897d",
   "metadata": {},
   "source": [
    "### 4b. Write functions to lemmatize and match texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbde7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    text = text.lower()\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\", \"textcat\"])\n",
    "    stp = nlp.Defaults.stop_words\n",
    "    lemma_no_stops = []\n",
    "    for char in punctuation:\n",
    "        if char != '\\'':\n",
    "            text = text.replace(char,\"\")\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        lemma = word.lemma_.lower()\n",
    "        if lemma not in stp:\n",
    "            lemma_no_stops.append(lemma)\n",
    "    return lemma_no_stops\n",
    "\n",
    "def match(summary, analysis):\n",
    "    x = process(summary)\n",
    "    y = ', '.join(list(analysis[0:10]['definition']))\n",
    "    z = process(y)\n",
    "    new = set() #intialize an empty set\n",
    "    for word in x:\n",
    "        if word in z:\n",
    "            new.add(word)\n",
    "    print('words in common:', len(new))\n",
    "    print('words:', new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b7425",
   "metadata": {},
   "source": [
    "### 4c. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dada8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\"Lishi's Diary\"\u001b[0m\n",
      "\n",
      "character-based frequency analysis:\n",
      "words in common: 5\n",
      "words: {'heart', 'letter', 'believe', 'return', 'day'}\n",
      "\n",
      "segement-based frequency analysis:\n",
      "words in common: 3\n",
      "words: {'tianjin', 'marry', 'night'}\n",
      "\n",
      "---------\n",
      "\u001b[1m\"Old Friends by the Seashore\"\u001b[0m\n",
      "character-based frequency analysis:\n",
      "words in common: 6\n",
      "words: {'heart', 'letter', 'believe', 'home', 'return', 'day'}\n",
      "\n",
      "segement-based frequency analysis:\n",
      "words in common: 2\n",
      "words: {'letter', 'friend'}\n",
      "\n",
      "---------\n",
      "\u001b[1m\"Drifting Women\"\u001b[0m\n",
      "\n",
      "character-based frequency analysis:\n",
      "words in common: 6\n",
      "words: {'woman', 'heart', 'man', 'love', 'home', 'return'}\n",
      "\n",
      "segement-based frequency analysis:\n",
      "words in common: 6\n",
      "words: {'marry', 'friend', 'man', 'home', 'return', 'find'}\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + '\"Lishi\\'s Diary\"' + '\\033[0m')\n",
    "\n",
    "print (\"\\ncharacter-based frequency analysis:\")\n",
    "match(lishi_summary, lishi_df)\n",
    "\n",
    "print (\"\\nsegement-based frequency analysis:\")\n",
    "match(lishi_summary, lishi_seg)\n",
    "\n",
    "print (\"\\n---------\")\n",
    "\n",
    "print('\\033[1m' + '\"Old Friends by the Seashore\"'+ '\\033[0m')\n",
    "print (\"character-based frequency analysis:\")\n",
    "match(lishi_summary, haibin_df)\n",
    "\n",
    "print (\"\\nsegement-based frequency analysis:\")\n",
    "match(lishi_summary, haibin_seg)\n",
    "\n",
    "print (\"\\n---------\")\n",
    "\n",
    "print('\\033[1m' + '\"Drifting Women\"'+ '\\033[0m')\n",
    "\n",
    "print (\"\\ncharacter-based frequency analysis:\")\n",
    "match(lishi_summary, piaobo_df)\n",
    "\n",
    "print (\"\\nsegement-based frequency analysis:\")\n",
    "match(lishi_summary, piaobo_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7188c",
   "metadata": {},
   "source": [
    "### 4d. Compare methods\n",
    "\n",
    "From this comparison of the analysis, we can see that the character-based frequency analysis actually seems to bring up more of the same words that are in the summaries for the first two stories. For \"Drifting Women,\" both seem equally helpful in terms of number of matched terms. We can see that the character-based frequency analysis tends to provide more conceptual words like \"believe,\" \"love,\" \"home,\" and \"heart,\" while the word-based gives more concrete and tangible words like \"friend\" and \"Tianjin.\" This tracks with how the Chinese language often combines characters that convey concepts into word pairs that hold a more specific semantic meaning. It's helpful to note that the matched terms are relatively low in the first place, so the summaries may note be the best way to evaluate the success of a word frequency analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50918300",
   "metadata": {},
   "source": [
    "# Exploring topic models with LDA\n",
    "\n",
    "Now I will try to use Latent Dirchlet Allocation to produce topic models to compare to the word frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f58654e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanziNLP in /opt/anaconda3/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: jieba>=0.42.1 in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (0.42.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (3.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (1.2.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (1.26.4)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (4.3.0)\n",
      "Requirement already satisfied: fasttext in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (0.9.3)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (4.41.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (2.3.1)\n",
      "Requirement already satisfied: ipywidgets>=7.6.3 in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (7.6.5)\n",
      "Requirement already satisfied: IPython>=7.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (8.20.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (0.12.2)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.11/site-packages (from HanziNLP) (5.9.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (5.7.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.11/site-packages (from IPython>=7.27.0->HanziNLP) (4.8.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets>=7.6.3->HanziNLP) (6.28.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets>=7.6.3->HanziNLP) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets>=7.6.3->HanziNLP) (5.9.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets>=7.6.3->HanziNLP) (3.5.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipywidgets>=7.6.3->HanziNLP) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.4.3->HanziNLP) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0->HanziNLP) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0->HanziNLP) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0->HanziNLP) (2.2.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/anaconda3/lib/python3.11/site-packages (from fasttext->HanziNLP) (2.13.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from fasttext->HanziNLP) (68.2.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->HanziNLP) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim->HanziNLP) (2.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->HanziNLP) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->HanziNLP) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from plotly->HanziNLP) (8.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch->HanziNLP) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers->HanziNLP) (4.65.0)\n",
      "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim->HanziNLP) (0.3.1)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (0.1.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (5.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/anaconda3/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (6.3.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->IPython>=7.27.0->HanziNLP) (0.8.3)\n",
      "Requirement already satisfied: fastjsonschema in /opt/anaconda3/lib/python3.11/site-packages (from nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/anaconda3/lib/python3.11/site-packages (from nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (4.19.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->IPython>=7.27.0->HanziNLP) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=7.27.0->HanziNLP) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.4.3->HanziNLP) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (6.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch->HanziNLP) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers->HanziNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers->HanziNLP) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers->HanziNLP) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers->HanziNLP) (2024.6.2)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.27.0->HanziNLP) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.27.0->HanziNLP) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.27.0->HanziNLP) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch->HanziNLP) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.6.3->HanziNLP) (0.10.6)\n",
      "Requirement already satisfied: entrypoints in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets>=7.6.3->HanziNLP) (3.10.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (21.3.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (7.10.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.14.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/anaconda3/lib/python3.11/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.0.0)\n",
      "Requirement already satisfied: simpful in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->HanziNLP) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->HanziNLP) (1.8.1)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/anaconda3/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.10.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/anaconda3/lib/python3.11/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/anaconda3/lib/python3.11/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/anaconda3/lib/python3.11/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (21.2.0)\n",
      "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->HanziNLP) (0.0.6)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (4.2.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.4.4)\n",
      "Requirement already satisfied: overrides in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (7.4.0)\n",
      "Requirement already satisfied: websocket-client in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.58.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.21)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (24.6.0)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.3->HanziNLP) (1.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install HanziNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042bde7",
   "metadata": {},
   "source": [
    "## 1. Using the LDA model to do topic modeling for a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c2b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning variable to open the file\n",
    "lishi = open('lishi.txt', encoding = 'utf-8').read() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90367c",
   "metadata": {},
   "source": [
    "Due to time constraints, I can only look at one of the texts with this method. I decided to look at \"Lishi's Diary.\" I'm using HanziNLP library's LDA function. https://github.com/samzshi0529/HanziNLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0a845e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHanziNLP\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_segment, word_tokenize, lda_model, print_topics\n\u001b[0;32m----> 3\u001b[0m text \u001b[38;5;241m=\u001b[39m ns(lishi)\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sentence_segment(text)\n\u001b[1;32m      5\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m [word_tokenize(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ns' is not defined"
     ]
    }
   ],
   "source": [
    "from HanziNLP import sentence_segment, word_tokenize, lda_model, print_topics\n",
    "\n",
    "text = ns(lishi)\n",
    "sentences = sentence_segment(text)\n",
    "tokenized_texts = [word_tokenize(sentence) for sentence in sentences]\n",
    "lda_model, corpus, dictionary = lda_model(tokenized_texts, num_topics=5, passes=100)\n",
    "print_topics(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c20caa",
   "metadata": {},
   "source": [
    "### Here's a better view of those topics with translations:\n",
    "\n",
    "#### Topic 0\n",
    "\"沅青\" Yuanqing (character), \"天津\" Tianjin (city name), \"憐\" pity, \"寂寞\" loneliness, lonely, \"麗石\" Lishi (character), \"約\" agree to meet, \"意思\" intention, \"路\" street, \"母親\" mother, \"同性\" same-gender\n",
    "\n",
    "#### Topic 1\n",
    "\"安慰\" comfort, \"生活\" life, \"早\" early, \"沅青\" Yuanqing (character), \"—\" em dash (punctuation), \"回想\" memory, \"奪\" seize, take away, \"類\" type, \"戴\" to wear, support, \"青\" youth\n",
    "\n",
    "#### Topic 2\n",
    "\"—\" em dash (punctuation), \"麗石\" Lishi (character), \"生\" life, birth, \"沅青\" Yuanqing (character), \"真\" real, true, \"床\" bed, \"病\" sickness, \"午\" noon, \"親愛\" beloved, darling, \"痕跡\" scars\n",
    "\n",
    "#### Topic 3\n",
    "\"沅青\" Yuanqing (character), \"天\" day, sky, heaven, \"話\" word, speech, \"記\" remember, record \"雯薇\" Wenwei (character), \"死\" die, death, \"正\" current, straight, correct, \"抑鬱\" depression, \"天覺\" sky + feel (not an expression) \"竟\" actually, unexpected\n",
    "\n",
    "#### Topic 4\n",
    "\"—\" em dash (punctuation), \"沅青\" Yuanqing (character) \"結婚\" marry, marriage, \"表兄\" cousin, \"酈文\" Liwen (character), \"聽\" hear, listen, \"覺\" feel, \"信\" believe, letter, \"恨\" hate, \"苦痛\" pain, suffering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3b613",
   "metadata": {},
   "source": [
    "## 2. Evaluating Topic Modeling\n",
    "\n",
    "The topic modeling was a little disappointing. These topics still feel like somewhat random collections of words, but when compared to the word frequency analysis, they are definitely better at showing us what the text is about. \n",
    "\n",
    "Topic 0 is probably the most random of all five. If I had to name it, I would call it \"Yuanqing and Lishi's sadness about location and family.\" \n",
    "\n",
    "Topic 1 is pretty good, and I might call this one \"concerns about youth and life.\" \n",
    "\n",
    "\n",
    "Topic 2 could be \"illness and love between Lishi and Yuanqing\"\n",
    "\n",
    "\n",
    "Topic 3 could be \"death and sadness connected to Yuanqing and Wenwei\"\n",
    "\n",
    "\n",
    "Topic 4 could be \"marriage, hate, and suffering connected to multiple characters\"\n",
    "\n",
    "I would say that all together, these three topic sum up the story reasonably well. But I already know the story and can see the connections, so I am biased to see the connections here. Once again we see that this text analysis strategy fails to really reliably predict the content of the text. \n",
    "\n",
    "#### The failure of LDA in this context is probably due to a few different factors:\n",
    "\n",
    "1. I did not preprocess the text, which is why there are weird things in the topics, such as em dashes. These should have been removed in the preprocessing stage, but when I tried to preprocess my texts, the topics did not come out well. This is something I don't understand and would want to troubleshoot in the future. \n",
    "\n",
    "2. This text is a short story, which means it's probably not ideal for using LDA. A much longer text would be better for topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ff502",
   "metadata": {},
   "source": [
    "# Project Conclusion\n",
    "\n",
    "From this project, I was able to familiarize myself with some basic tools of Chinese text analysis, such as the Python module jieba and LDA. I also learned how different languages require different computation approaches, such as segmentation vs. lemmatization, and character-based versus word-based analysis. I conducted different analyses and found that word frequency analysis can help get a rough picture of the important concepts in a text, but that overall it is not good enough to predict the content of a text. I conducted a second analysis with Latent Dirichlet Allocation, which produced five topics. When compared to the word frequency analysis, the LDA analysis did produce a better product, but it was still flawed. Fundamentally, all of these methods had flaws, in part due to user error, and in part due to the length of the texts I used. Still, I would say that these methods do provide an interesting look into the texts, which might not be as useful on it's own, but would be a good supplement to a close-reading analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
